\section{Related Work}
There has previously been some work related to database explanations as well as spatial analytics. 
In this section, we elaborate on the existing in such a field of study. 
%Since our automated framework leverages distributed computation frameworks, this section also elaborates the corresponding field.% Most of this work revolves around the notion of causality. There is existing work in the field of Artificial Intelligence by \cite{zhang2002discovering} which expresses relationships between different attributes in a dataset as conditional probabilities. Based on the conditional probability each tuple is given a set of binary rankings.

{\bf Causality in databases.} The work by \cite{meliou2010causality} is a survey which looks at causality from the perspective of a database problem. Traditionally, work on causality from the database perspective mainly deals with provenance i.e. events which occurred chronologically. This solution, however, only works with databases with timestamps. This paper also looks at the degree of responsibility which is defined as the number of tuples which have to be removed to change a binary observation. An example of a binary observation is winning and losing an election for instance. If a candidate wins by a high margin, each tuple has a lower degree of responsibility. On the other hand, a close victory for a candidate increases each voters degree of responsibility. The paper by \cite{meliou2014causality} is a survey of work in causality and explanations in both the Artificial Intelligence and Database communities. The take away from this survey is that AI problems tend to have a bigger causal network while database problems tend to have more variables.

{\bf Salient features correlation.} There has also been a lot of work on correlation which shares some common ground with the work on explanations. One interesting recent work on the subject is the Data Polygamy framework \cite{chirigati2016data}. This framework is designed to find the correlation between a corpus of datasets. It uses the peaks and troughs of the data to calculate \textit{salient features} \cite{dunn1986applied}. The positive and negative correlation between these salient features can be used to decide whether dataset are related \cite{su2014supporting}. 
%The objective of our system is a bit different from finding correlations. 
However, there are a number of different factors which can affect observations and correlation is only one of these attributes in certain cases. 
If we ignore other criteria such as selectivity then we might get results which do not have a significant impact. E.g. if two attributes are highly correlated in a certain spatial cluster but the selectivity of the cluster is small, it would lead to a low impact on the observation.

%{\bf Why not explanation} There has also been work related to why not explanations in databases. The work by \cite{ten2015high} looks at the question of why some tuples are missing from database results. This paper uses the assumption that the relationship between tuples is defined in the form of an ontology. The paper uses the relationship between the ontology for a schema and the ontology for an instance of the schema to judge whether an explanation exists. The ontologies that are used can be created manually or automatically. Using provenance \cite{cheney2009provenance} can help in creating ontologies.

{\bf Aggregation and intervention.} 
%Our solution mainly builds upon work by \cite{roy2014formal} which outlines a formal approach to explain data. 
The main solutions outlined in \cite{roy2014formal} are Aggravation and Intervention. 
It formally defines the {\fact} and {\explanation} in the problem of data explanation. 
The predicates on different attributes of the central table are exploited as the candidate {\explanation}s.
{\aggravation} and {\intervention} are defined based on the {\explanation} itself and the remaining data respectively. 
However, it is designed to work with non-spatial datasets. 
This method ignores the impact and characteristics of spatial data. 
So it does not translate well in the spatial domain and cannot find meaningful spatial {\explanation}s.
%One of the main points of the paper is that their approach works on a dataset which can span several tables related by primary and foreign keys. 
%While the approach outlined by \cite{roy2014formal} is great for non-spatial data, it does not translate well in the spatial domain. 
%This approach develops on previous work in causality and influence. It also resembles data mining concepts related to association rule mining \cite{agarwal1994fast,tan2006introduction}. 
%In association rule mining sets of attributes that occur together are assigned support and confidence. The support measures the frequency of occurrence of a set of attributes while confidence measures how frequently an attribute occurs with another set of attributes. The work by \cite{koperski1995discovery} looks at association rule mining in a spatial context. Given a set of spatial relationships, it applies association rule mining to find relationships that frequently occur together. There is a difference between association rule mining and our proposed approach. First of all, our proposed approach uses a user-defined observation. Secondly, in our approach, we are not looking at the associations but rather at the effect of removing or filtering pieces of data. A high association between predicates does not necessarily mean that removing them will significantly affect the observation.

%{\bf Spatial regression.} Spatial regression techniques can be used to explain data \cite{dunn1986applied,cleveland1988locally}. The idea behind regression techniques is to express an attribute that we are interested in as a dependent variable. This dependent variable can then be expressed in the form of a parametric equation involving other attributes in the dataset as independent variables. The user of this system decides a dependent variable and a set of explanation variables. The resulting equation has coefficients assigned to each explanatory variable as well as a bias term. This results in a curve fitting problem. The curve fitting problem is solved using regression i.e. the coefficient terms and the bias are iteratively adjusted until the sum of squared error between the predicted curve and the ground truth results in a minimum value. Regression techniques are widely used for spatial data analysis. However, they depend on predefined spatial partitioning and look at the data as a whole rather than from the perspective of a user-defined observation.
% Geographically Weighted Regression(GWR) expands on OLS regression for geospatial data \cite{brunsdon1998geographically,charlton2009geographically}. GWR tries to use the regression equation for each feature in the dataset. It uses the idea that spatially co-located points contribute more to each other by using a spatially aware kernel function. A kernel function like a Gaussian, for example, gives more weight to nearby points than to points which are far apart.

%{\bf Spatial autocorrelation.} Besides work related to explanation, there has been a lot of research in the area of spatial correlation. Much of the work in this area extends from multiple old works by Getis \cite{getis1991spatial,ord1995local,getis1996local,getis2002comparative,getis2007reflections}. The Getis Ord statistic \cite{ord1995local} for example is useful in showing us areas with high local spatial associations. The Moran's I statistic is useful for measuring the spatial heterogeneity of the data \cite{assuncao1999new,zhang2008use}. Moran's I is useful in hot spot analysis which can be viewed as a step in the way of finding explanations.


%\section{Big Spatial Data Systems.}
%{\bf Distributed data systems} Map Reduce \cite{dean2008mapreduce} is a framework for data processing which is designed for taking distributed and parallel computation into perspective. There are three main operations in a MapReduce process: map, shuffle and reduce. The map operation assigns a key to each element and performs any necessary transformations. The shuffle operation relocates the elements such that elements with the same key are nearby(since they are going to need each other in calculations). The reduce step performs a calculation on each element with the same key and returns the output. Spark \cite{shanahan2015large,zaharia2016apache} is a distributed and parallel processing framework. Spark uses a directed acyclic graph to perform calculations. Since the DAG created by Spark can have a lot of common nodes between tasks, the computational complexity of the operation is reduced compared to MapReduce.
%Geospark~ \cite{yu2015geospark} is a framework for performing several spatial operations on data in Apache Spark. It also has a component which helps in data visualizations \cite{yu2018src}.